__author__ = 'DrSaifKadim'
__author__ = 'DrSaifKadim'


# Load dataset
dataset = pd.read_csv("Put Face Feaure Dateset Here")

X = dataset.iloc[:,1:].values
y = dataset['class'].values


X= X.reshape(X.shape[0], X.shape[1], 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)

input_layer = Input(shape=(X_train.shape[1], 1))

# ========================
# Branch 1
branch1 = Conv1D(filters=16, kernel_size=3, strides=1, padding="same", activation="relu", name="conv1d_branch1_1")(input_layer)
branch1 = BatchNormalization(name="batch_norm_branch1_1")(branch1)
branch1 = Conv1D(filters=32, kernel_size=3, strides=1, padding="same", activation="relu", name="conv1d_branch1_2")(branch1)
branch1 = BatchNormalization(name="batch_norm_branch1_2")(branch1)
branch1 = Conv1D(filters=64, kernel_size=3, strides=1, padding="same", activation="relu", name="conv1d_branch1_3")(branch1)
branch1 = BatchNormalization(name="batch_norm_branch1_3")(branch1)
branch1 = MaxPooling1D(pool_size=2, strides=2, padding="same", name="maxpool_branch1")(branch1)
branch1 = Dropout(0.2, name="dropout_branch1")(branch1)

# ========================
# Branch 2
branch2 = Conv1D(filters=16, kernel_size=5, strides=1, padding="same", activation="relu", name="conv1d_branch2_1")(input_layer)
branch2 = BatchNormalization(name="batch_norm_branch2_1")(branch2)
branch2 = Conv1D(filters=32, kernel_size=5, strides=1, padding="same", activation="relu", name="conv1d_branch2_2")(branch2)
branch2 = BatchNormalization(name="batch_norm_branch2_2")(branch2)
branch2 = Conv1D(filters=64, kernel_size=5, strides=1, padding="same", activation="relu", name="conv1d_branch2_3")(branch2)
branch2 = BatchNormalization(name="batch_norm_branch2_3")(branch2)
branch2 = MaxPooling1D(pool_size=2, strides=2, padding="same", name="maxpool_branch2")(branch2)
branch2 = Dropout(0.2, name="dropout_branch2")(branch2)

# ========================
# Branch 3
branch3 = Conv1D(filters=8, kernel_size=7, strides=1, padding="same", activation="relu", name="conv1d_branch3_1")(input_layer)
branch3 = BatchNormalization(name="batch_norm_branch3_1")(branch3)
branch3 = Conv1D(filters=16, kernel_size=7, strides=1, padding="same", activation="relu", name="conv1d_branch3_2")(branch3)
branch3 = BatchNormalization(name="batch_norm_branch3_2")(branch3)
branch3 = Conv1D(filters=32, kernel_size=7, strides=1, padding="same", activation="relu", name="conv1d_branch3_3")(branch3)
branch3 = BatchNormalization(name="batch_norm_branch3_3")(branch3)
branch3 = MaxPooling1D(pool_size=2, strides=2, padding="same", name="maxpool_branch3")(branch3)
branch3 = Dropout(0.2, name="dropout_branch3")(branch3)

# ========================
# Branch 4
branch4 = Conv1D(filters=32, kernel_size=2, strides=1, padding="same", activation="relu", name="conv1d_branch4_1")(input_layer)
branch4 = BatchNormalization(name="batch_norm_branch4_1")(branch4)
branch4 = Conv1D(filters=64, kernel_size=2, strides=1, padding="same", activation="relu", name="conv1d_branch4_2")(branch4)
branch4 = BatchNormalization(name="batch_norm_branch4_2")(branch4)
branch4 = Conv1D(filters=128, kernel_size=2, strides=1, padding="same", activation="relu", name="conv1d_branch4_3")(branch4)
branch4 = BatchNormalization(name="batch_norm_branch4_3")(branch4)
branch4 = MaxPooling1D(pool_size=2, strides=2, padding="same", name="maxpool_branch4")(branch4)
branch4 = Dropout(0.2, name="dropout_branch4")(branch4)

# ========================
# Branch 5
branch5 = Conv1D(filters=16, kernel_size=4, strides=1, padding="same", activation="relu", name="conv1d_branch5_1")(input_layer)
branch5 = BatchNormalization(name="batch_norm_branch5_1")(branch5)
branch5 = Conv1D(filters=32, kernel_size=4, strides=1, padding="same", activation="relu", name="conv1d_branch5_2")(branch5)
branch5 = BatchNormalization(name="batch_norm_branch5_2")(branch5)
branch5 = Conv1D(filters=64, kernel_size=4, strides=1, padding="same", activation="relu", name="conv1d_branch5_3")(branch5)
branch5 = BatchNormalization(name="batch_norm_branch5_3")(branch5)
branch5 = MaxPooling1D(pool_size=2, strides=2, padding="same", name="maxpool_branch5")(branch5)
branch5 = Dropout(0.2, name="dropout_branch5")(branch5)

# ========================
# Concatenating All Branches
concat = Concatenate(name="concatenate_branches")([branch1, branch2, branch3, branch4, branch5])

# ========================
# GRU Layers after Concatenation
gru1 = GRU(16, return_sequences=True, name="gru1")(concat)
gru2 = GRU(8, return_sequences=False, name="gru2")(gru1)
gru2 = Dropout(0.2, name="dropout_gru2")(gru2)


gru3 = GRU(16, return_sequences=True, name="gru3")(concat)
gru4 = GRU(8, return_sequences=False, name="gru4")(gru3)
gru4 = Dropout(0.2, name="dropout_gru3")(gru4)


gru5 = GRU(16, return_sequences=True, name="gru5")(concat)
gru6 = GRU(8, return_sequences=False, name="gru6")(gru5)
gru6 = Dropout(0.2, name="dropout_gru4")(gru6)

concat1 = Concatenate(name="concatenate_branches1")([gru2, gru4,gru6])

# ========================
# Fully Connected Layers after GRU
fc1 = Dense(128, activation="relu", name="fc1")(concat1)
fc1 = BatchNormalization(name="batch_norm_fc1")(fc1)
fc1 = Dropout(0.4, name="dropout_fc1")(fc1)

fc2 = Dense(64, activation="relu", name="fc2")(fc1)
fc2 = BatchNormalization(name="batch_norm_fc2")(fc2)
fc2 = Dropout(0.4, name="dropout_fc2")(fc2)

fc3 = Dense(32, activation="relu", name="fc3")(fc2)
fc3 = BatchNormalization(name="batch_norm_fc3")(fc3)

# ========================
# Output Layer (Softmax)
output_layer = Dense(276, activation="softmax", name="output_layer")(fc3)

# ========================
# Create the model
model = Model(inputs=input_layer, outputs=output_layer)

model.summary()

# Compile and train model
opt = optimizers.Adam(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the model
y_pred = model.predict(X_test)


cr = classification_report(y_test, y_pred_bool)
print(cr)
# Get training history
history = model.history.history

# Plot training loss
plt.plot(history['loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training accuracy
plt.plot(history['acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
